{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model...\n",
      "No labels provided. Feature importance based methods are not available.\n",
      "14 features with a correlation magnitude greater than 0.85.\n",
      "\n",
      "Removed 14 features.\n",
      "1.13.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "shap_values() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e47cc8d665be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: shap_values() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend\n",
    "from keras.models import model_from_json\n",
    "import sys\n",
    "import time\n",
    "from dataset_prepare import prepare_data\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils as npu\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "from IPython.display import display\n",
    "import pandas as pd \n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from feature_selector import FeatureSelector\n",
    "\n",
    "def clean_nominals_and_create_our_datasets(train_set_df,test_set_df):\n",
    "    #Get nominal columns\n",
    "    nominal_cols = train_set_df.select_dtypes(include='object').columns.tolist()\n",
    "    #Turn nominal to numeric train and testX\n",
    "    for nom in nominal_cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_set_df[nom])\n",
    "        train_set_df[nom]=le.transform(train_set_df[nom])\n",
    "        testEnc = LabelEncoder()\n",
    "        testEnc.classes_=le.classes_\n",
    "        testEnc.fit(test_set_df[nom])\n",
    "        test_set_df[nom] =testEnc.transform(test_set_df[nom])\n",
    "        #print(dict(zip(testEnc.classes_, testEnc.transform(testEnc.classes_))))\n",
    "    classes = dict(zip(testEnc.classes_, testEnc.transform(testEnc.classes_)))\n",
    "\n",
    "    # drop the nominal columns from the initial set\n",
    "    train_set_df_y = train_set_df.attack_cat\n",
    "    train_Y = np.array(train_set_df_y)\n",
    "    train_Y = train_Y.reshape((train_Y.shape[0],1))\n",
    "    train_set_df = train_set_df.drop([\"attack_cat\",\"label\"], axis=1)\n",
    "    train_X = np.array(train_set_df)\n",
    "\n",
    "    test_set_df_y = test_set_df.attack_cat\n",
    "    test_set_df_y1 = test_set_df.label\n",
    "    test_Y = np.array(test_set_df_y)\n",
    "    test_Y = test_Y.reshape((test_Y.shape[0],1))\n",
    "    test_set_df = test_set_df.drop([\"attack_cat\",\"label\"], axis=1)\n",
    "    test_X = np.array(test_set_df)\n",
    "\n",
    "    labels = np.array(test_set_df_y1)\n",
    "    labels = labels.reshape((labels.shape[0],1))\n",
    "    \n",
    "    return train_X,train_Y,test_X,test_Y,test_set_df,labels, classes\n",
    "\n",
    "def prepare_data(): \n",
    "    \"\"\"\n",
    "    This function is the main of this module. calls the above functions in order to read/clean/save\n",
    "    our data in usable form.\n",
    "    I created this function to use dataset_prepare.py as a Python module in our main program.\n",
    "    \n",
    "    Return values: training X,Y dataset and testing X,Y dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # read our csv files\n",
    "    features_df = pd.read_csv(\"UNSW_NB15_features.csv\",encoding = \"ISO-8859-1\")\n",
    "    training_df = pd.read_csv(\"training.csv\").drop(\"id\",axis=1)\n",
    "    testing_df = pd.read_csv(\"testing.csv\").drop(\"id\",axis=1)\n",
    "\n",
    "    fs = FeatureSelector(data = training_df)\n",
    "    fs.identify_collinear(correlation_threshold=0.85)\n",
    "    training_df = fs.remove(methods = ['collinear'],keep_one_hot = True)\n",
    "    columnlist = list(training_df)\n",
    "    testing_df = testing_df[columnlist]\n",
    "    \n",
    "    training_df = training_df.sample(frac=1)\n",
    "    testing_df = testing_df.sample(frac=1)\n",
    "    train_x,train_y,test_x,test_y, testing_df, labels, classes = clean_nominals_and_create_our_datasets(training_df,testing_df)\n",
    "\n",
    "    training_df = training_df.drop([\"attack_cat\",\"label\"], axis=1)\n",
    "    training_df = np.array(list(training_df))\n",
    "    #print(\"The features we will use are: \")\n",
    "    #for i in range(len(training_df)):\n",
    "    #    print(\"[\"+ str(i) +\"]: \" + str(training_df[i]))\n",
    "\n",
    "    return train_x,train_y,test_x,test_y,testing_df,labels, classes, training_df\n",
    "\n",
    "def load_model(name):\n",
    "    #load json and create model\n",
    "    json_file = open(name+\".json\",\"r\")\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "    model.load_weights(name+\".h5\")\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(\"Loaded model...\")\n",
    "    return model\n",
    "\n",
    "model = load_model('idsmodel')\n",
    "train_x,train_y,test_x,test_y ,testxdf, labels, classes, traindf= prepare_data()\n",
    "train_y = train_y.reshape((train_y.shape[0],))\n",
    "print(tensorflow.__version__)\n",
    "explainer = shap.DeepExplainer(model, train_x)\n",
    "\n",
    "shap_values = explainer.shap_values(test_x[:10])\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "classnames = [x for x in classes.keys()]\n",
    "display(shap.summary_plot(shap_values, testxdf, class_names=classnames))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
